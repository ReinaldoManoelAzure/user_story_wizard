import streamlit as st
from dotenv import load_dotenv
import os
import json
import xml.etree.ElementTree as ET
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from io import BytesIO
from docx import Document
from markdown_pdf import MarkdownPdf, Section

# carregar vari√°veis do .env
load_dotenv()

st.set_page_config(page_title="User Story Wizard", layout="wide")

# -------------------
# Fun√ß√£o para processar arquivos (mini-RAG)
# -------------------
def process_files(uploaded_files):
    conteudo = []
    for file in uploaded_files:
        if file.name.endswith(".pdf"):
            loader = PyPDFLoader(file)
            docs = loader.load()
            conteudo.extend([d.page_content for d in docs])

        elif file.name.endswith(".docx"):
            loader = Docx2txtLoader(file)
            docs = loader.load()
            conteudo.extend([d.page_content for d in docs])

        elif file.name.endswith(".txt"):
            text = file.read().decode("utf-8")
            conteudo.append(text)

        elif file.name.endswith(".json"):
            text = json.dumps(json.load(file), indent=2, ensure_ascii=False)
            conteudo.append(text)

        elif file.name.endswith(".xml"):
            tree = ET.parse(file)
            root = tree.getroot()
            text = ET.tostring(root, encoding="utf-8").decode("utf-8")
            conteudo.append(text)

    return "\n\n".join(conteudo)


# -------------------
# Fun√ß√£o para criar a hist√≥ria com fallback de modelos
# -------------------
def build_with_fallback(contexto_texto, macro, inputs, arquivos_contexto):
    prompt = ChatPromptTemplate.from_template("""
    Construa uma **Hist√≥ria Funcional** no seguinte formato:

    Hist√≥ria Funcional

    T√≠tulo: <T√≠tulo claro e objetivo>

    Como
    <persona ou papel do usu√°rio>

    Quero
    <objetivo ou a√ß√£o desejada>

    Para
    <benef√≠cio ou valor agregado>

    Crit√©rios de Aceite
    - Liste crit√©rios claros e verific√°veis

    Regras de Neg√≥cio
    - Liste as regras de neg√≥cio aplic√°veis

    Informa√ß√µes T√©cnicas
    **Itens Priorit√°rios (informados pelo usu√°rio em 'Informa√ß√µes T√©cnicas ou Observa√ß√µes Extras'):**
    - Liste diretamente os itens fornecidos pelo usu√°rio. 
    - Se o usu√°rio n√£o informou nada, escreva "Nenhum informado."

    **Propostas Adicionais (sugeridas pelo modelo a partir de contexto e arquivos):**
    - Sugira pontos t√©cnicos adicionais que possam ser relevantes. 
    - Essas propostas devem ser opcionais, n√£o obrigat√≥rias.

    ---  
    **Contexto Macro:**  
    {macro}

    **Contexto Detalhado:**  
    {contexto_texto}

    **Inputs adicionais:**  
    {inputs}

    **Informa√ß√µes extra√≠das de arquivos:**  
    {arquivos_contexto}
    """)

    # üîÑ Fallback: Gemini -> OpenAI
    try:
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0.1,
            api_key=os.getenv("GOOGLE_API_KEY")
        )
        chain = prompt | llm
        response = chain.invoke({
            "macro": macro,
            "contexto_texto": contexto_texto,
            "inputs": inputs,
            "arquivos_contexto": arquivos_contexto
        })
        return f"{response.content}"

    except Exception as e1:
        try:
            llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)
            chain = prompt | llm
            response = chain.invoke({
                "macro": macro,
                "contexto_texto": contexto_texto,
                "inputs": inputs,
                "arquivos_contexto": arquivos_contexto
            })
            return f"(‚ö†Ô∏è Gemini falhou, fallback para OpenAI GPT-4o-mini)\n\n{response.content}"
        except Exception as e2:
            return f"‚ùå Nenhum modelo p√¥de ser utilizado.\nErro Gemini: {str(e1)}\nErro OpenAI: {str(e2)}"


# -------------------
# Export helpers
# -------------------
def export_docx(story_text: str) -> BytesIO:
    doc = Document()
    for line in story_text.split("\n"):
        if line.startswith("T√≠tulo:"):
            doc.add_heading(line.replace("T√≠tulo:", "").strip(), level=1)
        elif line.startswith("Como"):
            doc.add_heading("Como", level=2)
            doc.add_paragraph(line.replace("Como", "").strip())
        elif line.startswith("Quero"):
            doc.add_heading("Quero", level=2)
            doc.add_paragraph(line.replace("Quero", "").strip())
        elif line.startswith("Para"):
            doc.add_heading("Para", level=2)
            doc.add_paragraph(line.replace("Para", "").strip())
        elif line.strip().endswith(":"):
            doc.add_heading(line.strip(), level=2)
        elif line.strip().startswith("-"):
            doc.add_paragraph(line.strip(), style="List Bullet")
        else:
            doc.add_paragraph(line.strip())
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer


def export_pdf(story_text: str) -> BytesIO:
    buffer = BytesIO()
    pdf = MarkdownPdf()
    pdf.add_section(Section(story_text, toc=False))
    pdf.save(buffer)
    buffer.seek(0)
    return buffer


# -------------------
# UI do Streamlit
# -------------------
def main():
    st.title("üßô User Story Wizard")
    st.markdown("Monte hist√≥rias de usu√°rio de forma padronizada.")

    # üìñ Explica√ß√£o educativa
    with st.expander("‚ÑπÔ∏è O que √© uma Hist√≥ria de Usu√°rio?"):
        st.markdown("""
        Uma **Hist√≥ria de Usu√°rio** √© uma forma simples e clara de descrever uma necessidade de neg√≥cio do ponto de vista do usu√°rio final.  
        
        Ela geralmente segue o formato:

        **Como [persona]**  
        **Quero [a√ß√£o ou objetivo]**  
        **Para [benef√≠cio ou valor]**
        """)

    st.subheader("üåç Qual √© o cen√°rio geral do projeto?")
    macro = st.text_area(
        "Objetivo geral do sistema",
        height=120,
        placeholder="Sistema de gest√£o de contratos para empresas de log√≠stica, com integra√ß√£o ao ERP."
    )

    st.subheader("üìñ Detalhe o que precisa ser feito")
    contexto_texto = st.text_area(
        "Requisitos e descri√ß√µes do cliente",
        height=200,
        placeholder="""Usu√°rios precisam visualizar contratos ativos e vencidos, receber notifica√ß√µes autom√°ticas de renova√ß√£o e gerar relat√≥rios detalhados por cliente e filial. 
Gestores administrativos ter√£o acesso a relat√≥rios financeiros consolidados."""
    )

    st.subheader("üìé Anexe documentos de apoio")
    arquivos = st.file_uploader(
        "Envie arquivos (docx, pdf, txt, xml, json)",
        type=["docx", "pdf", "txt", "xml", "json"],
        accept_multiple_files=True
    )
    
    arquivos_contexto = ""
    if arquivos:
        arquivos_contexto = process_files(arquivos)

    st.subheader("üîß Informa√ß√µes T√©cnicas ou Observa√ß√µes Extras")
    inputs = st.text_area(
        "Digite restri√ß√µes t√©cnicas, regras espec√≠ficas ou observa√ß√µes adicionais",
        height=100,
        placeholder="Compat√≠vel com Chrome e Edge.\nAPI deve responder em at√© 2 segundos.",
        help="Se voc√™ n√£o preencher nada, o modelo ainda sugerir√° **Propostas Adicionais** automaticamente."
    )
    st.caption("üí° Exemplo: 'Compat√≠vel com Chrome e Edge. API deve responder em at√© 2 segundos.'")

    # -------------------
    # Gera√ß√£o da hist√≥ria
    # -------------------
    if st.button("‚ú® Criar Hist√≥ria Funcional"):
        if not macro and not contexto_texto and not arquivos_contexto:
            st.warning("Por favor, preencha algum campo de contexto ou envie arquivos.")
        else:
            with st.spinner("Gerando hist√≥ria..."):
                st.session_state["story"] = build_with_fallback(
                    contexto_texto, macro, inputs, arquivos_contexto
                )
            st.success("‚úÖ Hist√≥ria gerada com sucesso!")

    # -------------------
    # Exibir resultado + bot√µes de download
    # -------------------
    if "story" in st.session_state and st.session_state["story"]:
        story = st.session_state["story"]
        st.markdown("### üìñ Resultado")
        st.markdown(story)

        st.download_button(
            label="üì• Baixar DOCX",
            data=export_docx(story),
            file_name="historia_funcional.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        st.download_button(
            label="üì• Baixar PDF",
            data=export_pdf(story),
            file_name="historia_funcional.pdf",
            mime="application/pdf"
        )


if __name__ == "__main__":
    main()
